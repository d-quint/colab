{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b902d7fa",
   "metadata": {},
   "source": [
    "# YOLO Training Interface in Google Colab\n",
    "This notebook demonstrates how to set up and run the YOLO Training Interface in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0b8ca",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e49429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required dependencies\n",
    "!python -m pip install ultralytics>=8.0.0 rich>=10.0.0 pyyaml>=6.0 pandas>=1.3.0 matplotlib>=3.4.0 numpy>=1.20.0 ray>=2.0.0 gdown>=4.6.0 opencv-python>=4.5.0\n",
    "\n",
    "# Import necessary packages for this notebook\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af0924",
   "metadata": {},
   "source": [
    "## Step 2: Upload the required files\n",
    "Upload the following files to this Colab session:\n",
    "1. program.py - The main YOLO training interface\n",
    "2. setup_env.py - The environment setup script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c750bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files directly from Github repository\n",
    "!wget https://raw.githubusercontent.com/d-quint/colab/refs/heads/main/program.py -O program.py\n",
    "!wget https://raw.githubusercontent.com/d-quint/colab/refs/heads/main/setup_env.py -O setup_env.py\n",
    "\n",
    "# Verify the files were downloaded correctly\n",
    "!ls -la program.py setup_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df681667",
   "metadata": {},
   "source": [
    "## Step 3: Set up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the setup script to create the necessary directory structure\n",
    "%run setup_env.py --download_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097dc957",
   "metadata": {},
   "source": [
    "## Fix Sample Model File\n",
    "\n",
    "The setup script creates a placeholder .pt file that isn't a real model. Let's replace it with a real YOLOv8 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the sample weight file by replacing it with a real YOLOv8 model\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Directory for the sample model\n",
    "sample_weights_dir = \"/content/drive/MyDrive/YOLO_Training/runs/detect/yolov8n_sample/weights\"\n",
    "sample_weights_file = os.path.join(sample_weights_dir, \"last.pt\")\n",
    "\n",
    "# Check if the sample weight file exists and is just a placeholder\n",
    "if os.path.exists(sample_weights_file):\n",
    "    with open(sample_weights_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        if content.startswith(\"#\"):\n",
    "            print(\"Detected placeholder model file. Replacing with a real YOLOv8n model...\")\n",
    "            \n",
    "            # Download a real YOLOv8n model\n",
    "            model = YOLO(\"yolov8n.pt\")\n",
    "            \n",
    "            # Save the real model to the sample weights directory\n",
    "            os.remove(sample_weights_file)  # Remove the placeholder\n",
    "            model.save(sample_weights_file)  # Save the real model\n",
    "            \n",
    "            print(f\"✅ Successfully replaced placeholder with real model at: {sample_weights_file}\")\n",
    "else:\n",
    "    print(f\"Sample weights directory not found. Creating it...\")\n",
    "    os.makedirs(sample_weights_dir, exist_ok=True)\n",
    "    \n",
    "    # Download and save a real YOLOv8n model\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "    model.save(sample_weights_file)\n",
    "    \n",
    "    print(f\"✅ Created real model at: {sample_weights_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ab208",
   "metadata": {},
   "source": [
    "## Step 4: Download Pretrained Model\n",
    "Download a pretrained YOLO model from Google Drive. This model (new3ktrash_tip) contains:\n",
    "- `args.yaml` - The arguments/configuration used for training the model\n",
    "- `weights/best.pt` - The trained model weights\n",
    "\n",
    "We'll add this model to the YOLO environment so it can be used for inference or further training.\n",
    "Note: This is a custom trained model, not just a dataset. After downloading, we'll be able to use it for inference on new images or fine-tune it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained model from Google Drive using gdown\n",
    "import gdown\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Google Drive file ID for the new3ktrash_tip model\n",
    "file_id = \"19Mzi4Ueqyh3i634c9oUvP4NEskwoB86z\"\n",
    "output_path = \"pretrained_model.zip\"\n",
    "model_name = \"new3ktrash_tip\"  # Name of the model when extracted\n",
    "\n",
    "# Target directory in the runs/detect folder where models are stored\n",
    "model_dir = f\"/content/drive/MyDrive/YOLO_Training/runs/detect/{model_name}\"\n",
    "model_weights_dir = os.path.join(model_dir, \"weights\")\n",
    "\n",
    "# Check if the model is already downloaded\n",
    "if os.path.exists(os.path.join(model_weights_dir, \"best.pt\")):\n",
    "    print(f\"✅ Model {model_name} is already downloaded and ready to use.\")\n",
    "else:\n",
    "    # Download the file\n",
    "    print(f\"Downloading pretrained model '{model_name}' from Google Drive...\")\n",
    "    gdown.download(id=file_id, output=output_path, quiet=False)\n",
    "    \n",
    "    # Extract the model\n",
    "    print(\"\\nExtracting the model files...\")\n",
    "    \n",
    "    # Create the target directories\n",
    "    os.makedirs(model_weights_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract the model archive\n",
    "    !unzip -q {output_path} -d /tmp/model_extract\n",
    "    \n",
    "    # Check what was extracted and move it to the correct location\n",
    "    extract_dir = \"/tmp/model_extract\"\n",
    "    extract_contents = os.listdir(extract_dir)\n",
    "    print(f\"Extracted contents: {extract_contents}\")\n",
    "    \n",
    "    # Handle different archive structures\n",
    "    if model_name in extract_contents:\n",
    "        # If the archive contains a directory with the model name\n",
    "        src_dir = os.path.join(extract_dir, model_name)\n",
    "        if os.path.isdir(src_dir):\n",
    "            print(f\"Moving files from {src_dir} to {model_dir}\")\n",
    "            !cp -r {src_dir}/* {model_dir}\n",
    "    else:\n",
    "        # If the archive extracts directly to files\n",
    "        print(f\"Moving all extracted files to {model_dir}\")\n",
    "        !cp -r {extract_dir}/* {model_dir}\n",
    "    \n",
    "    # Clean up the temporary files\n",
    "    !rm {output_path}\n",
    "    !rm -rf /tmp/model_extract\n",
    "\n",
    "# Verify the model structure\n",
    "print(f\"\\nVerifying downloaded model structure:\")\n",
    "!ls -la {model_dir}\n",
    "\n",
    "# Check for weights directory\n",
    "if os.path.exists(model_weights_dir):\n",
    "    print(\"\\nModel weights directory:\")\n",
    "    !ls -la {model_weights_dir}\n",
    "else:\n",
    "    print(\"\\n❌ Warning: Weights directory not found!\")\n",
    "    # Try to find weight files in the main directory\n",
    "    weight_files = [f for f in os.listdir(model_dir) if f.endswith('.pt')]\n",
    "    if weight_files:\n",
    "        print(f\"Found weight files in main directory: {weight_files}\")\n",
    "        # Create weights directory and move files\n",
    "        os.makedirs(model_weights_dir, exist_ok=True)\n",
    "        for wf in weight_files:\n",
    "            shutil.move(os.path.join(model_dir, wf), os.path.join(model_weights_dir, wf))\n",
    "        print(\"✅ Moved weight files to weights directory\")\n",
    "        !ls -la {model_weights_dir}\n",
    "\n",
    "# Verify the key files exist\n",
    "args_yaml = os.path.join(model_dir, \"args.yaml\")\n",
    "best_weights = os.path.join(model_weights_dir, \"best.pt\")\n",
    "last_weights = os.path.join(model_weights_dir, \"last.pt\")\n",
    "\n",
    "# Check for configuration file\n",
    "if os.path.exists(args_yaml):\n",
    "    print(f\"\\n✅ Found model configuration: {args_yaml}\")\n",
    "    # Display the configuration\n",
    "    with open(args_yaml, 'r') as f:\n",
    "        try:\n",
    "            config = yaml.safe_load(f)\n",
    "            print(\"\\nModel Configuration:\")\n",
    "            for key, value in config.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading config: {str(e)}\")\n",
    "else:\n",
    "    print(f\"\\n❌ Warning: Model configuration not found at: {args_yaml}\")\n",
    "\n",
    "# Check for weight files\n",
    "weight_found = False\n",
    "if os.path.exists(best_weights):\n",
    "    print(f\"✅ Found best model weights: {best_weights}\")\n",
    "    primary_weights = best_weights\n",
    "    weight_found = True\n",
    "elif os.path.exists(last_weights):\n",
    "    print(f\"✅ Found last model weights: {last_weights}\")\n",
    "    primary_weights = last_weights\n",
    "    weight_found = True\n",
    "else:\n",
    "    # Check for any PT files\n",
    "    weight_files = [f for f in os.listdir(model_weights_dir) if f.endswith('.pt')]\n",
    "    if weight_files:\n",
    "        primary_weights = os.path.join(model_weights_dir, weight_files[0])\n",
    "        print(f\"✅ Found alternative weight file: {weight_files[0]}\")\n",
    "        weight_found = True\n",
    "    else:\n",
    "        print(f\"❌ Error: No model weight files found!\")\n",
    "        primary_weights = None\n",
    "\n",
    "# Save paths for later use\n",
    "if weight_found:\n",
    "    # Save the model path for easy access in other cells\n",
    "    model_path_file = \"/content/drive/MyDrive/YOLO_Training/model_path.txt\"\n",
    "    with open(model_path_file, 'w') as f:\n",
    "        f.write(primary_weights)\n",
    "    print(f\"\\n✅ Model is ready to use at: {primary_weights}\")\n",
    "    print(f\"   Model path saved to: {model_path_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the dataset.yaml file to use the downloaded trash dataset\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Path where we want to save our main dataset.yaml\n",
    "dataset_yaml = \"/content/drive/MyDrive/YOLO_Training/dataset.yaml\"\n",
    "\n",
    "# Define dataset directories\n",
    "trash_dataset_dir = \"/content/drive/MyDrive/YOLO_Training/datasets/trash_dataset\"\n",
    "custom_dataset_dir = \"/content/drive/MyDrive/YOLO_Training/datasets/custom_dataset\"\n",
    "\n",
    "# First, try to use the trash dataset that matches our pretrained model\n",
    "if os.path.exists(trash_dataset_dir) and os.path.exists(os.path.join(trash_dataset_dir, \"data.yaml\")):\n",
    "    print(f\"Using trash dataset that matches the pretrained model: {trash_dataset_dir}\")\n",
    "    trash_data_yaml = os.path.join(trash_dataset_dir, \"data.yaml\")\n",
    "    \n",
    "    # Read the data.yaml from the downloaded trash dataset\n",
    "    with open(trash_data_yaml, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    \n",
    "    # Update paths to be absolute\n",
    "    data['path'] = trash_dataset_dir\n",
    "    \n",
    "    # Save to our main dataset.yaml location\n",
    "    with open(dataset_yaml, 'w') as f:\n",
    "        yaml.dump(data, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"Updated {dataset_yaml} to use trash dataset\")\n",
    "    print(\"\\nDataset configuration:\")\n",
    "    with open(dataset_yaml, 'r') as f:\n",
    "        print(f.read())\n",
    "    \n",
    "    print(\"\\n✅ The dataset is now properly configured for the pretrained model\")\n",
    "\n",
    "# Fallback to custom dataset if trash dataset not found\n",
    "elif os.path.exists(custom_dataset_dir):\n",
    "    print(f\"Trash dataset not found. Using custom dataset: {custom_dataset_dir}\")\n",
    "    \n",
    "    # Look for yaml files in the custom dataset directory\n",
    "    yaml_files = [f for f in os.listdir(custom_dataset_dir) if f.endswith('.yaml')]\n",
    "    \n",
    "    if yaml_files:\n",
    "        # Use the first yaml file found\n",
    "        dataset_config_path = os.path.join(custom_dataset_dir, yaml_files[0])\n",
    "        print(f\"Found dataset config: {dataset_config_path}\")\n",
    "        \n",
    "        # Read the yaml file\n",
    "        with open(dataset_config_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        # Update paths to be absolute\n",
    "        data['path'] = custom_dataset_dir\n",
    "        \n",
    "        # Save to our main dataset.yaml location\n",
    "        with open(dataset_yaml, 'w') as f:\n",
    "            yaml.dump(data, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"Updated {dataset_yaml} with custom dataset configuration\")\n",
    "        print(\"\\nDataset configuration:\")\n",
    "        with open(dataset_yaml, 'r') as f:\n",
    "            print(f.read())\n",
    "    else:\n",
    "        print(f\"No YAML configuration found. Creating a default dataset.yaml file.\")\n",
    "        \n",
    "        # Create a default dataset.yaml\n",
    "        default_config = {\n",
    "            'path': custom_dataset_dir,\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/val',\n",
    "            'test': 'images/test',\n",
    "            'names': {\n",
    "                0: 'object',  # Default class name\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(dataset_yaml, 'w') as f:\n",
    "            yaml.dump(default_config, f, default_flow_style=False)\n",
    "        \n",
    "        print(\"\\nDefault dataset configuration created:\")\n",
    "        with open(dataset_yaml, 'r') as f:\n",
    "            print(f.read())\n",
    "        \n",
    "        print(\"\\nNote: You should update this default configuration with your actual classes.\")\n",
    "\n",
    "# Create custom_dataset directory if neither dataset exists\n",
    "else:\n",
    "    print(f\"Neither trash dataset nor custom dataset found. Creating directory structure...\")\n",
    "    \n",
    "    # Create the custom dataset directory structure\n",
    "    os.makedirs(custom_dataset_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(custom_dataset_dir, \"images\", \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(custom_dataset_dir, \"images\", \"val\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(custom_dataset_dir, \"images\", \"test\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(custom_dataset_dir, \"labels\", \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(custom_dataset_dir, \"labels\", \"val\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(custom_dataset_dir, \"labels\", \"test\"), exist_ok=True)\n",
    "    \n",
    "    # Create a default dataset.yaml\n",
    "    default_config = {\n",
    "        'path': custom_dataset_dir,\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'test': 'images/test',\n",
    "        'names': {\n",
    "            0: 'object',  # Default class name\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(dataset_yaml, 'w') as f:\n",
    "        yaml.dump(default_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(\"\\n✅ Created sample dataset directory structure\")\n",
    "    print(\"Default dataset configuration created:\")\n",
    "    with open(dataset_yaml, 'r') as f:\n",
    "        print(f.read())\n",
    "    \n",
    "    print(\"\\nWARNING: You should download the trash dataset with the command:\")\n",
    "    print('curl -L \"https://app.roboflow.com/ds/9oNEAZK93B?key=BjWc8uu2z1\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip')\n",
    "    print(\"This is needed for optimal performance with the pretrained model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c50db9",
   "metadata": {},
   "source": [
    "## Step 5: Run the YOLO Training Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the directory where the program.py is located\n",
    "%cd /content/drive/MyDrive/YOLO_Training\n",
    "\n",
    "# Run the program\n",
    "%run program.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20e850",
   "metadata": {},
   "source": [
    "## Optional: Monitor Training with TensorBoard\n",
    "YOLOv8 automatically creates TensorBoard logs which you can view in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f82eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard\n",
    "%tensorboard --logdir /content/drive/MyDrive/YOLO_Training/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08afba8",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Error: \"invalid load key, '#'\"\n",
    "\n",
    "If you encounter the error message \"invalid load key, '#'\" when trying to run hyperparameter tuning, this typically happens because the setup script creates placeholder model files instead of real ones. The fix is included in this notebook (see \"Fix Sample Model File\" section above).\n",
    "\n",
    "If you still encounter this issue:\n",
    "\n",
    "1. Check if the model file is a real PyTorch model and not a placeholder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model file is valid\n",
    "def check_model_file(model_path):\n",
    "    try:\n",
    "        # Try opening as text\n",
    "        with open(model_path, 'r') as f:\n",
    "            content = f.read(100)  # Read just the first 100 chars\n",
    "            \n",
    "            # If it starts with a # character, it's a placeholder text file\n",
    "            if content.strip().startswith('#'):\n",
    "                print(f\"❌ Error: {model_path} is a placeholder text file, not a real model.\")\n",
    "                print(\"Content preview:\")\n",
    "                print(content)\n",
    "                return False\n",
    "    except UnicodeDecodeError:\n",
    "        # If we can't read it as text, it's likely a binary file (good!)\n",
    "        print(f\"✅ {model_path} appears to be a binary file (likely a real model).\")\n",
    "        return True\n",
    "        \n",
    "    # Final check - try loading with YOLO\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "        model = YOLO(model_path)\n",
    "        print(f\"✅ Successfully loaded {model_path} as a YOLO model.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model with YOLO: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Check the sample model file\n",
    "sample_model_path = \"/content/drive/MyDrive/YOLO_Training/runs/detect/yolov8n_sample/weights/last.pt\"\n",
    "if os.path.exists(sample_model_path):\n",
    "    check_model_file(sample_model_path)\n",
    "else:\n",
    "    print(f\"Model file not found: {sample_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69fe66c",
   "metadata": {},
   "source": [
    "2. If the model file is invalid, replace it with a real one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace an invalid model file with a real one\n",
    "def replace_invalid_model(model_path):\n",
    "    print(f\"Replacing invalid model at {model_path} with a real YOLOv8n model...\")\n",
    "    \n",
    "    # Make sure directory exists\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    \n",
    "    # Download a real YOLOv8n model\n",
    "    from ultralytics import YOLO\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "    \n",
    "    # Remove old file if it exists\n",
    "    if os.path.exists(model_path):\n",
    "        os.remove(model_path)\n",
    "        \n",
    "    # Save the real model\n",
    "    model.save(model_path)\n",
    "    print(f\"✅ Successfully replaced with real model at: {model_path}\")\n",
    "    return True\n",
    "\n",
    "# Uncomment and run if needed\n",
    "# replace_invalid_model(\"/content/drive/MyDrive/YOLO_Training/runs/detect/yolov8n_sample/weights/last.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58421b",
   "metadata": {},
   "source": [
    "## Step 4.3: Test the Downloaded Model\n",
    "Let's verify the downloaded model works by performing inference on a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the downloaded model with a sample image\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Check if we have the model path from the previous step\n",
    "try:\n",
    "    with open(\"/content/drive/MyDrive/YOLO_Training/model_path.txt\", 'r') as f:\n",
    "        model_path = f.read().strip()\n",
    "        print(f\"Using model from: {model_path}\")\n",
    "except FileNotFoundError:\n",
    "    # Use default path if file not found\n",
    "    model_path = f\"/content/drive/MyDrive/YOLO_Training/runs/detect/new3ktrash_tip/weights/best.pt\"\n",
    "    print(f\"Using default model path: {model_path}\")\n",
    "\n",
    "# Check if model exists\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"❌ Error: Model not found at {model_path}\")\n",
    "    # Try to find alternatives\n",
    "    model_dir = os.path.dirname(os.path.dirname(model_path))\n",
    "    weights_dir = os.path.join(model_dir, \"weights\")\n",
    "    \n",
    "    if os.path.exists(weights_dir):\n",
    "        weight_files = [f for f in os.listdir(weights_dir) if f.endswith('.pt')]\n",
    "        if weight_files:\n",
    "            model_path = os.path.join(weights_dir, weight_files[0])\n",
    "            print(f\"Found alternative model: {model_path}\")\n",
    "        else:\n",
    "            print(\"No model weight files found!\")\n",
    "    else:\n",
    "        print(f\"Weights directory not found at {weights_dir}\")\n",
    "\n",
    "# Load the model\n",
    "try:\n",
    "    model = YOLO(model_path)\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    \n",
    "    # Download a sample image for testing\n",
    "    !wget -q https://ultralytics.com/images/bus.jpg -O sample_image.jpg\n",
    "    \n",
    "    # Display the image\n",
    "    img = cv2.imread('sample_image.jpg')\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(\"Sample Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Run inference\n",
    "    print(\"\\nRunning inference on sample image...\")\n",
    "    results = model('sample_image.jpg')\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nDetection Results:\")\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        print(f\"Found {len(boxes)} objects\")\n",
    "        \n",
    "        # Get class names from the model\n",
    "        class_names = result.names\n",
    "        print(\"Classes detected:\", [class_names[int(cls)] for cls in boxes.cls.cpu().numpy()])\n",
    "    \n",
    "    # Plot detection results\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    result_img = results[0].plot()\n",
    "    plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Detection Results\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✅ Model test complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading or using the model: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Make sure the model file exists and is a valid YOLO model\")\n",
    "    print(\"2. Check if the model format is compatible with your version of ultralytics\")\n",
    "    print(\"3. Try downloading the model again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f450e4",
   "metadata": {},
   "source": [
    "## Step 4.4: Using the Pretrained Model in the Training Interface\n",
    "\n",
    "When running the YOLO Training Interface in Step 5, you can use the downloaded model for:\n",
    "\n",
    "1. **Inference**: Selecting the model in the interface to run predictions on new images\n",
    "2. **Fine-tuning**: Further training the model on your own dataset\n",
    "3. **Hyperparameter tuning**: Optimizing the model parameters for better performance\n",
    "\n",
    "To use the downloaded model in the training interface:\n",
    "\n",
    "1. When prompted to select a base model, choose **\"Use previously trained custom model\"**\n",
    "2. Select the **\"new3ktrash_tip\"** directory from the list\n",
    "3. Choose the weight file (usually **\"best.pt\"**) from the weights directory\n",
    "\n",
    "You can also access the model directly in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d50f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to load and use the downloaded model in your own code\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the model (this will use the path we saved earlier)\n",
    "try:\n",
    "    with open(\"/content/drive/MyDrive/YOLO_Training/model_path.txt\", 'r') as f:\n",
    "        model_path = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    model_path = \"/content/drive/MyDrive/YOLO_Training/runs/detect/new3ktrash_tip/weights/best.pt\"\n",
    "\n",
    "# Create a YOLO model instance\n",
    "model = YOLO(model_path)\n",
    "\n",
    "print(f\"Model loaded from: {model_path}\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"1. Run inference on an image:\")\n",
    "print('   results = model(\"path/to/image.jpg\")')\n",
    "print(\"2. Fine-tune the model on a new dataset:\")\n",
    "print('   results = model.train(data=\"dataset.yaml\", epochs=50)')\n",
    "print(\"3. Export the model for deployment:\")\n",
    "print('   success = model.export(format=\"onnx\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb827a",
   "metadata": {},
   "source": [
    "### Error: \"Access denied with the following error: ...\"\n",
    "\n",
    "If you encounter access denied errors when downloading from Google Drive:\n",
    "\n",
    "1. The file sharing settings might have changed\n",
    "2. Google Drive may be rate-limiting your downloads\n",
    "\n",
    "Solutions:\n",
    "- Try using the direct Google Drive web interface to download the file\n",
    "- Create a shortcut to the file in your own Google Drive, then download from there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f004c9c",
   "metadata": {},
   "source": [
    "### Error: \"Model file not found\" or \"No model weight files found\"\n",
    "\n",
    "If the model weights cannot be found after downloading:\n",
    "\n",
    "1. Check the extracted directory structure\n",
    "2. The archive might have a different structure than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe69433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to manually locate and fix model files\n",
    "def find_and_fix_model_files():\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    model_name = \"new3ktrash_tip\"\n",
    "    base_dir = \"/content/drive/MyDrive/YOLO_Training\"\n",
    "    model_dir = os.path.join(base_dir, \"runs\", \"detect\", model_name)\n",
    "    \n",
    "    print(f\"Searching for model files in {model_dir}...\")\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f\"❌ Model directory not found: {model_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Look for weights\n",
    "    weight_files = []\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.pt'):\n",
    "                weight_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if weight_files:\n",
    "        print(f\"✅ Found {len(weight_files)} weight files:\")\n",
    "        for i, wf in enumerate(weight_files):\n",
    "            print(f\"  [{i+1}] {wf}\")\n",
    "        \n",
    "        # Create weights dir if needed\n",
    "        weights_dir = os.path.join(model_dir, \"weights\")\n",
    "        os.makedirs(weights_dir, exist_ok=True)\n",
    "        \n",
    "        # Move files that aren't already in the weights directory\n",
    "        for wf in weight_files:\n",
    "            if os.path.dirname(wf) != weights_dir:\n",
    "                new_path = os.path.join(weights_dir, os.path.basename(wf))\n",
    "                print(f\"Moving {wf} → {new_path}\")\n",
    "                \n",
    "                if not os.path.exists(new_path):\n",
    "                    import shutil\n",
    "                    shutil.copy2(wf, new_path)\n",
    "                    print(f\"  ✅ Copied to weights directory\")\n",
    "        \n",
    "        # Verify weights directory\n",
    "        print(f\"\\nWeights directory contents:\")\n",
    "        if os.path.exists(weights_dir):\n",
    "            print(os.listdir(weights_dir))\n",
    "        else:\n",
    "            print(\"❌ Weights directory still not found\")\n",
    "    else:\n",
    "        print(\"❌ No weight files found anywhere in the model directory!\")\n",
    "        \n",
    "    # Check for args.yaml\n",
    "    args_yaml = os.path.join(model_dir, \"args.yaml\")\n",
    "    if os.path.exists(args_yaml):\n",
    "        print(f\"✅ Found args.yaml file: {args_yaml}\")\n",
    "    else:\n",
    "        yaml_files = []\n",
    "        for root, dirs, files in os.walk(model_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.yaml'):\n",
    "                    yaml_files.append(os.path.join(root, file))\n",
    "        \n",
    "        if yaml_files:\n",
    "            print(f\"✅ Found {len(yaml_files)} YAML files:\")\n",
    "            for i, yf in enumerate(yaml_files):\n",
    "                print(f\"  [{i+1}] {yf}\")\n",
    "            \n",
    "            # Copy the first yaml file to args.yaml if it doesn't exist\n",
    "            import shutil\n",
    "            shutil.copy2(yaml_files[0], args_yaml)\n",
    "            print(f\"  ✅ Copied {yaml_files[0]} to {args_yaml}\")\n",
    "        else:\n",
    "            print(\"❌ No YAML configuration files found!\")\n",
    "            \n",
    "            # Create a basic args.yaml\n",
    "            import yaml\n",
    "            basic_args = {\n",
    "                'task': 'detect',\n",
    "                'model': model_name,\n",
    "                'data': 'dataset.yaml',\n",
    "                'epochs': 100,\n",
    "                'patience': 50,\n",
    "                'batch': 16,\n",
    "                'imgsz': 640,\n",
    "                'save': True,\n",
    "                'exist_ok': False,\n",
    "                'name': model_name\n",
    "            }\n",
    "            \n",
    "            with open(args_yaml, 'w') as f:\n",
    "                yaml.dump(basic_args, f, default_flow_style=False)\n",
    "            print(f\"  ✅ Created a basic args.yaml file\")\n",
    "\n",
    "# Uncomment to run this function if you're having trouble with model files\n",
    "# find_and_fix_model_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15734683",
   "metadata": {},
   "source": [
    "## Step 4.1: Download Dataset for Pretrained Model\n",
    "Let's download the specific dataset from Roboflow that matches our pretrained \"new3ktrash_tip\" model. This dataset is needed for proper evaluation or further training with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and set up the dataset that matches our pretrained model\n",
    "import os\n",
    "\n",
    "# Define the dataset directory\n",
    "dataset_dir = \"/content/drive/MyDrive/YOLO_Training/datasets/trash_dataset\"\n",
    "\n",
    "# Check if dataset already exists\n",
    "if os.path.exists(dataset_dir) and os.path.exists(os.path.join(dataset_dir, \"data.yaml\")):\n",
    "    print(f\"✅ Dataset already exists at: {dataset_dir}\")\n",
    "else:\n",
    "    print(\"Downloading the dataset that matches our pretrained model...\")\n",
    "    \n",
    "    # Create dataset directory if it doesn't exist\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    \n",
    "    # Change to the dataset directory\n",
    "    %cd {dataset_dir}\n",
    "    \n",
    "    # Download and extract the dataset using curl\n",
    "    !curl -L \"https://app.roboflow.com/ds/9oNEAZK93B?key=BjWc8uu2z1\" > roboflow.zip\n",
    "    !unzip -q roboflow.zip\n",
    "    !rm roboflow.zip\n",
    "    \n",
    "    # Return to the main directory\n",
    "    %cd /content/drive/MyDrive/YOLO_Training\n",
    "    \n",
    "    print(f\"✅ Dataset downloaded to: {dataset_dir}\")\n",
    "    \n",
    "# Check the dataset structure\n",
    "if os.path.exists(dataset_dir):\n",
    "    print(\"\\nDataset structure:\")\n",
    "    !ls -la {dataset_dir}\n",
    "    \n",
    "    # Check if data.yaml exists\n",
    "    data_yaml = os.path.join(dataset_dir, \"data.yaml\")\n",
    "    if os.path.exists(data_yaml):\n",
    "        print(f\"\\nFound dataset configuration: {data_yaml}\")\n",
    "        \n",
    "        # Read and display dataset configuration\n",
    "        with open(data_yaml, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            print(\"\\nDataset configuration:\")\n",
    "            print(f\"- Classes: {len(config.get('names', []))} ({config.get('names')})\")\n",
    "            print(f\"- Train images: {config.get('train', 'unknown')}\")\n",
    "            print(f\"- Validation images: {config.get('val', 'unknown')}\")\n",
    "            print(f\"- Test images: {config.get('test', 'unknown')}\")\n",
    "    else:\n",
    "        print(f\"❌ Dataset configuration file (data.yaml) not found!\")\n",
    "else:\n",
    "    print(f\"❌ Dataset directory not found: {dataset_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
